1. Use the given link Data Set.https://archive.ics.uci.edu/ml/datasets/Yeast
Answer the below questions:

a. What are the assumptions of ANOVA, test it out?

Assumptions and Effects of Violating Assumptions 

1 Deviation from Normal Distribution 
2 Homogeneity of Variances 
3 Homogeneity of Variances and Covariances 
4 Sphericity and Compound Symmetry 

DEVIATION FROM NORMAL DISTRIBUTION 

Assumptions. It is assumed that the dependent variable is measured on at least an interval scale level . Moreover, the dependent variable should be normally distributed within groups. 

Effects of violations. Overall, the F test is remarkably robust to deviations from normality . If the kurtosis is greater than 0, then the F tends to be too small and we cannot reject the null hypothesis even though it is incorrect. The opposite is the case when the kurtosis is less than 0. The skewness of the distribution usually does not have a sizable effect on the F statistic. If the nper cell is fairly large, then deviations from normality do not matter much at all because of the central limit theorem, according to which the sampling distribution of the mean approximates the normal distribution, regardless of the distribution of the variable in the population. A detailed discussion of the robustness of the F statistic can be found in Box and Anderson (1955), or Lindman (1974). 

HOMOGENEITY OF VARIANCES 

Assumptions. It is assumed that the variances in the different groups of the design are identical; this assumption is called the homogeneity of variances assumption. Remember that at the beginning of this section we computed the error variance (SS error) by adding up the sums of squares within each group. If the variances in the two groups are different from each other, then adding the two together is not appropriate, and will not yield an estimate of the common within-group variance (since no common variance exists).


Special case: correlated means and variances. However, one instance when the F statistic is very misleading is when the means are correlated with variances across cells of the design. A scatterplot of variances or standard deviations against the means will detect such correlations. The reason why this is a "dangerous" violation is the following: Imagine that we have 8 cells in the design, 7 with about equal means but one with a much higher mean. The F statistic may suggest a statistically significant effect. However, suppose that there also is a much larger variance in the cell with the highest mean, that is, the means and the variances are correlated across cells (the higher the mean the larger the variance). In that case, the high mean in the one cell is actually quite unreliable, as is indicated by the large variance. However, because the overall F statistic is based on a pooled within-cell variance estimate, the high mean is identified as significantly different from the others, when in fact it is not at all significantly different if we based the test on the within-cell variance in that cell alone. This pattern - a high mean and a large variance in one cell - frequently occurs when there are outliers present in the data. One or two extreme cases in a cell with only 10 cases can greatly bias the mean, and will dramatically increase the variance.



# 1. Homogeneity of variances 
# 2. Normality 

#Normality plot of residuals. In the plot below, the quantiles of the residuals are plotted against the quantiles of the normal distribution. A 45-degree reference line is also plotted. 
#The normal probability plot of residuals is used to check the assumption that the residuals are normally distributed. It should approximately follow a straight line. 

plot(res.aov, 1)

plot(res.aov, 2)

plot(res.aov, 3)

plot(res.aov, 4)

plot(res.aov, 5)

plot(res.aov, 6)






b. Why ANOVA test? Is there any other way to answer the above question?



Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the "variation" among and between groups) used to analyze the differences among group mean in a sample. ANOVA was developed by statistician and evolutionary biologist Ronald Fisher.

Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the "variation" among and between groups) used to analyze the differences among group mean in a sample. ANOVA was developed by statistician and evolutionary biologist Ronald Fisher. The Analysis of Variance table is just like any other ANOVA table. The Total Sum of Squares is the uncertainty that would be present if one had to predict individual responses without any other information. The best one could do is predict each observation to be equal to the overall sample mean. The ANOVA table partitions this variability into two parts. One portion is accounted for (some say "explained by") the model. It's the reduction in uncertainty that occurs when the ANOVA model, is fitted to the data. The remaining portion is the uncertainty that remains even after the model is used. The model is considered to be statistically significant if it can account for a large amount of variability in the response.


Model, Error, Corrected Total, Sum of Squares, Degrees of Freedom, F Value, and Pr F have the same meanings as for multiple regression. This is to be expected since analysis of variance is nothing more than the regression of the response on a set of indicators definded by the categorical predictor variable. The degrees of freedom for the model is equal to one less than the number of categories. The F ratio is nothing more than the extra sum of squares principle applied to the full set of indicator variables defined by the categorical predictor variable. The F ratio and its P value are the same regardless of the particular set of indicators that is used.

Sums of Squares: The total amount of variability in the response can be written as the sum of the squared differences between each observation and the overall mean. If we were asked to make a prediction without any other information, the best we can do, in a certain sense, is the overall mean. The amount of variation in the data that can't be accounted for by this simple method of prediction is the Total Sum of Squares.


When the Analysis of Variance model is used for prediction, the best that can be done is to predict each observation to be equal to its group's mean. The amount of uncertainty that remains is sum of the squared differences between each observation and its group's mean. This is the Error sum of squares. In this outpur it also appears as the GROUP sum of squares. The difference between the Total sum of squares and the Error sum of squares is the Model Sum of Squares which happens to be equal to sum of the squared differences between each observation and its group's mean multiplied by ni

Each sum of squares has corresponding degrees of freedom (DF) associated with it. Total df is one less than the number of observations, N-1. The Model df is the one less than the number of levels The Error df is the difference between the Total df (N-1) and the Model df (g-1), that is, N-g. Another way to calculate the error degrees of freedom is by summing up the error degrees of freedom from each group, ni-1, over all g groups.

The Mean Squares are the Sums of Squares divided by the corresponding degrees of freedom. N-g. Another way to calculate the error degrees of freedom is by summing up the error degrees of freedom from each group, ni-1, over all g groups.

The Mean Squares are the Sums of Squares divided by the corresponding degrees of freedom.

The F Value or F ratio is the test statistic used to decide whether the sample means are within sampling variability of each other. That is, it tests the hypothesis H0: mu 1 ....mu g. This is the same thing as asking whether the model as a whole has statistically significant predictive capability in the regression framework. F is the ratio of the Model Mean Square to the Error Mean Square. Under the null hypothesis that the model has no predictive capability--that is, that all of the population means are equal--the F statistic follows an F distribution with p numerator degrees of freedom and n-p-1 denominator degrees of freedom. The null hypothesis is rejected if the F ratio is large. This statistics and P value might be ignored depending on the primary research question and whether a multiple comparisons procedure is used


Other Methodology to test:


# Extract the residuals 
aov_residuals <- residuals(object = res.aov ) 
# Run Shapiro-Wilk test 
shapiro.test(x = aov_residuals )


kruskal.test(nuc ~ mcg, data = yeast)

#The classical one-way ANOVA test requires an assumption of equal variances for all groups.An alternative procedure (i.e.: Welch one-way test), that does not require that assumption have been implemented in the function oneway.test(). 

#The assumptions on which f-test relies are: 
#The population is normally distributed. 
#Samples have been drawn randomly. 
#Observations are independent. 
#H0 may be one sided or two sided. 

oneway.test(nuc ~ LocalizationSite , data = yeast)

#Pairewise t-test 
#The function pairewise.t.test() can be also used to calculate pairwise comparisons between group levels with corrections for multiple testing. 

pairwise.t.test(yeast$nuc, yeast$LocalizationSite, p.adjust.method = "BH")


#The result is a table of p-values for the pairwise comparisons. Here, the p-values have been adjusted by the Benjamini-Hochberg method. 

#One-Way ANOVA with Pairwise Comparisons. Whereas a one-way omnibus ANOVA assesses whether a significant difference exists at all amongst the groups, pairwise comparisons can be used to determine which group differences are statistically significant. 

#Roughly, paired t-test is a t-test in which each subject is compared with itself or, in other words, determines whether they differ from each other in a significant way under the assumptions that the paired differences are independent and identically normally distributed.
